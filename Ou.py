import json
import re
import requests
from typing import Dict

from langchain_community.llms import Ollama
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import InMemoryChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.runnables.history import RunnableWithHistory

# === CONFIG ===
API_URL = "http://localhost:8000/projetos"
MODEL_NAME = "qwen2:4b"  # ou o modelo que estiver usando
OLLAMA_URL = "http://localhost:11434"
# O ID da sess√£o agora √© passado como um argumento, tornando o app mais flex√≠vel
DEFAULT_SESSION_ID = "sessao_padrao_1"

# === Documenta√ß√£o da API ===
API_DOC = """
A API retorna uma lista de registros no formato:
[
  {
    "id": n√∫mero identificador do registro,
    "overallStatus": status geral do projeto,
    "modelCategory": categoria do modelo (ex: smartphone, tablet),
    "plmDevModelName": nome do modelo no sistema PLM,
    "marketModelName": nome do modelo para o mercado,
    "laPia": data do evento LA PIA (AAAA-MM-DD),
    "laPra": data do evento LA PRA (AAAA-MM-DD),
    "laSra": data do evento LA SRA (AAAA-MM-DD),
    "laPsa": data do evento LA PSA (AAAA-MM-DD),
    "bbModelName": nome do modelo Backbone relacionado,
    "osversion": vers√£o do sistema operacional
  }
]
"""

# === Fun√ß√£o para carregar dados reais da API ===
def carregar_dados_api():
    """Carrega os dados da API com tratamento de erros."""
    try:
        # Adicionado um timeout para evitar que o programa trave indefinidamente
        resp = requests.get(API_URL, timeout=10)
        resp.raise_for_status()  # Lan√ßa um erro para status HTTP 4xx/5xx
        return resp.json()
    except requests.exceptions.RequestException as e:
        print(f"Erro ao acessar a API: {e}")
        # Retorna uma lista vazia para que o resto do script n√£o quebre
        return []

dados_api = carregar_dados_api()
# Garante que mesmo com dados vazios, o JSON seja uma lista vazia '[]'
dados_json = json.dumps(dados_api, ensure_ascii=False, indent=2) if dados_api else "[]"

# === Cat√°logo de modelos (para identificar 'last_model') ===
def build_model_catalog(dados):
    """Constr√≥i um cat√°logo de nomes de modelo e uma regex para encontr√°-los."""
    if not dados:
        return set(), None
    nomes = set()
    for r in dados:
        for k in ("plmDevModelName", "marketModelName"):
            v = (r.get(k) or "").strip()
            if v:
                nomes.add(v)
    
    # Ordena por comprimento para garantir que nomes mais longos (ex: "Galaxy S25 Ultra")
    # sejam encontrados antes de nomes mais curtos (ex: "Galaxy S25")
    nomes_ordenados = sorted(list(nomes), key=len, reverse=True)
    
    if not nomes_ordenados:
        return set(), None
        
    # Escapa caracteres especiais para a regex
    pattern = re.compile("|".join(map(re.escape, nomes_ordenados)), flags=re.IGNORECASE)
    return set(nomes_ordenados), pattern

CATALOGO_MODELOS, REGEX_MODELOS = build_model_catalog(dados_api)

# === GERENCIAMENTO DE ESTADO POR SESS√ÉO (A GRANDE MELHORIA) ===
# Em vez de vari√°veis globais, usamos dicion√°rios para armazenar o estado de cada sess√£o.
# Isso permite que m√∫ltiplas conversas ocorram simultaneamente sem interfer√™ncia.
store: Dict[str, BaseChatMessageHistory] = {}
last_model_store: Dict[str, str] = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """Cria ou recupera um hist√≥rico de chat para a sess√£o informada."""
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]

def get_last_model(session_id: str) -> str:
    """Recupera o √∫ltimo modelo consultado para a sess√£o."""
    return last_model_store.get(session_id, "Nenhum")

def set_last_model(session_id: str, model_name: str):
    """Define o √∫ltimo modelo consultado para a sess√£o."""
    if REGEX_MODELOS and model_name:
        # Normaliza o nome do modelo para o nome exato do cat√°logo para consist√™ncia
        match = REGEX_MODELOS.search(model_name)
        if match:
            last_model_store[session_id] = match.group(0)

# === LLM ===
llm = Ollama(model=MODEL_NAME, temperature=0, base_url=OLLAMA_URL)
output_parser = StrOutputParser()

# === Prompt (Modernizado) ===
# Usamos MessagesPlaceholder para indicar onde o hist√≥rico da conversa deve ser inserido.
# O RunnableWithHistory ir√° preencher isso automaticamente.
prompt_template = ChatPromptTemplate.from_messages([
    ("system", f"""
Voc√™ √© um assistente que responde perguntas sobre registros de uma API.
Responda SOMENTE com base nos dados fornecidos abaixo.
Se a informa√ß√£o n√£o estiver nos dados, responda exatamente: "N√£o encontrei essa informa√ß√£o."
Sempre que poss√≠vel, formate a resposta de forma clara (listas, tabelas).

üìñ Documenta√ß√£o da API:
{API_DOC}

üìä Dados dispon√≠veis:
{dados_json}
    """),
    # Este placeholder ser√° preenchido pelo RunnableWithHistory
    MessagesPlaceholder(variable_name="history"),
    # A l√≥gica de last_model √© personalizada, ent√£o a mantemos
    ("system", "Contexto Adicional: O √∫ltimo modelo que est√°vamos discutindo √©: {last_model}"),
    ("human", "{input}")
])

# === Chain (Simplificada) ===
# Removemos a atribui√ß√£o manual de 'history'. A chain agora s√≥ precisa
# saber como obter o 'last_model' para a sess√£o atual.
chain = (
    RunnablePassthrough.assign(
        # O session_id √© acessado a partir do 'config' passado para a chain.
        last_model=lambda x: get_last_model(x["config"]["configurable"]["session_id"])
    )
    | prompt_template
    | llm
    | output_parser
)

# Encapsula com hist√≥rico usando o novo padr√£o
chain_with_history = RunnableWithHistory(
    chain,
    get_session_history,
    input_messages_key="input",       # A chave da pergunta do usu√°rio
    history_messages_key="history",   # A chave do placeholder no prompt
)

# === Fun√ß√£o de atualiza√ß√£o ===
def atualizar_last_model(session_id: str, pergunta: str, resposta: str):
    """Busca por nomes de modelo na pergunta e resposta para atualizar o contexto da sess√£o."""
    if REGEX_MODELOS:
        # Busca primeiro na pergunta, que tem maior probabilidade de ser expl√≠cita
        match = REGEX_MODELOS.search(pergunta)
        if match:
            set_last_model(session_id, match.group(0))
            return # Encontrou na pergunta, n√£o precisa olhar a resposta
        
        # Se n√£o encontrou na pergunta, busca na resposta do LLM
        match = REGEX_MODELOS.search(resposta)
        if match:
            set_last_model(session_id, match.group(0))

# === Fun√ß√£o principal de chat ===
def chat(pergunta: str, session_id: str):
    """Executa a chain de conversa√ß√£o para uma dada sess√£o."""
    print(f"\n--- [Sess√£o: {session_id}] ---")
    print(f"üë§ Usu√°rio: {pergunta}")
    
    # O config √© essencial para o RunnableWithHistory saber qual hist√≥rico usar.
    config = {"configurable": {"session_id": session_id}}
    
    resposta_completa = ""
    print("ü§ñ Assistente: ", end="", flush=True)
    for token in chain_with_history.stream({"input": pergunta}, config=config):
        print(token, end="", flush=True)
        resposta_completa += token
    print() # Nova linha no final

    # Atualiza o √∫ltimo modelo consultado para esta sess√£o
    atualizar_last_model(session_id, pergunta, resposta_completa)
    
    # Opcional: imprimir o estado atual para depura√ß√£o
    # print(f"DEBUG: Hist√≥rico da sess√£o '{session_id}': {get_session_history(session_id).messages}")
    # print(f"DEBUG: √öltimo modelo da sess√£o '{session_id}': {get_last_model(session_id)}")


# === Exemplo de uso ===
if __name__ == "__main__":
    print("Iniciando o chatbot. Os dados da API foram carregados.")
    if not dados_api:
        print("AVISO: N√£o foi poss√≠vel carregar dados da API. O chatbot responder√° com base em dados vazios.")
    
    # Exemplo de conversa com a sess√£o 1
    chat("Qual a data LA PIA do modelo Galaxy S25 Ultra?", session_id="sessao1")
    chat("E qual a vers√£o do sistema operacional dele?", session_id="sessao1") # Pergunta de acompanhamento
    
    # Exemplo de conversa com a sess√£o 2, totalmente independente
    chat("Me liste os modelos da categoria smartphone", session_id="sessao2")
    chat("Qual o status do Vision Pro?", session_id="sessao2")
    chat("e o PRA dele?", session_id="sessao2") # Pergunta de acompanhamento sobre Vision Pro

    # Voltando para a sess√£o 1 para mostrar que a mem√≥ria foi mantida
    chat("Obrigado pelas informa√ß√µes sobre o S25 Ultra.", session_id="sessao1")

