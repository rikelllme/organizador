# Importa√ß√µes necess√°rias
from langchain_community.llms import Ollama
from langchain.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# NOVAS importa√ß√µes para a estrat√©gia RAG
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

import requests
import json

# === CONFIGURA√á√ïES ===
API_URL = "http://localhost:8000/projetos"
MODEL_NAME = "qwen2:4b" 
OLLAMA_URL = "http://localhost:11434"

# Fun√ß√£o para buscar dados da API (sem altera√ß√µes)
def carregar_dados_api():
    """Carrega os dados da API com tratamento de erros."""
    try:
        resp = requests.get(API_URL, timeout=10)
        resp.raise_for_status()
        return resp.json()
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erro ao buscar dados da API: {e}")
        return []

# === ETAPA 1: INDEXA√á√ÉO (O "LER PRIMEIRO") ===

print("üîÑ Carregando dados da API...")
dados_api = carregar_dados_api()

# Se os dados foram carregados com sucesso, criamos o Vector Store
if dados_api:
    print("‚úÖ Dados carregados. Iniciando a indexa√ß√£o para busca r√°pida...")
    
    # Converte cada objeto JSON em uma string para ser processada
    documentos_texto = [json.dumps(item, ensure_ascii=False) for item in dados_api]
    
    # 1. Splitter: Quebra os textos em peda√ßos menores se necess√°rio (bom para textos longos)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
    docs_split = text_splitter.create_documents(documentos_texto)

    # 2. Embeddings: Modelo que transforma texto em vetores num√©ricos
    embeddings = OllamaEmbeddings(model=MODEL_NAME, base_url=OLLAMA_URL)

    # 3. Vector Store: Cria o "√≠ndice de busca" em mem√≥ria usando FAISS
    #    Este processo converte todos os peda√ßos de documento em vetores e os armazena.
    vector_store = FAISS.from_documents(docs_split, embeddings)

    # 4. Retriever: Objeto que sabe como buscar informa√ß√µes no Vector Store
    retriever = vector_store.as_retriever(search_kwargs={"k": 3}) # Pega os 3 resultados mais relevantes
    
    print("‚úÖ Indexa√ß√£o completa! O assistente est√° pronto.")
else:
    print("‚ùå N√£o foi poss√≠vel carregar dados da API. Encerrando o programa.")
    exit()


# === ETAPA 2: CONFIGURA√á√ÉO DA CHAIN DE RESPOSTA ===

# 1. Modelo LLM e Mem√≥ria (sem altera√ß√µes)
llm = Ollama(model=MODEL_NAME, temperature=0, base_url=OLLAMA_URL)
memory = ConversationBufferMemory(memory_key="history", input_key="input", return_messages=True)

# 2. NOVO Prompt Template
#    Agora ele n√£o recebe mais o JSON inteiro. Ele recebe uma vari√°vel "context"
#    que conter√° apenas os trechos relevantes encontrados pelo retriever.
prompt_template = ChatPromptTemplate.from_messages([
    ("system", """
Voc√™ √© um assistente especialista em responder perguntas com base em um contexto fornecido.
Responda √† pergunta do usu√°rio SOMENTE com base nas informa√ß√µes abaixo.
Se a resposta n√£o estiver no contexto, diga: "N√£o encontrei essa informa√ß√£o nos dados relevantes."

Contexto Relevante:
{context}
    """),
    ("system", "Hist√≥rico da conversa anterior:\n{history}"),
    ("human", "{input}")
])

# 3. Parser de sa√≠da (sem altera√ß√µes)
output_parser = StrOutputParser()

# 4. NOVA Chain de execu√ß√£o com RAG
#    O fluxo agora √© mais inteligente:
#    a. A pergunta do usu√°rio ("input") √© passada para o retriever.
#    b. O retriever busca no Vector Store o contexto relevante.
#    c. O contexto, a pergunta e o hist√≥rico s√£o passados para o prompt.
#    d. O prompt formatado vai para o LLM, que gera a resposta.
chain = (
    {
        "context": (lambda x: x['input']) | retriever, # Pega o input, busca o contexto
        "input": RunnablePassthrough(), # Passa o dicion√°rio de entrada inteiro adiante
        "history": lambda x: memory.load_memory_variables(x).get("history")
    }
    | {
        "context": lambda x: x['context'],
        "input": lambda x: x['input']['input'], # Extrai a string do input
        "history": lambda x: x['history']
    }
    | prompt_template
    | llm
    | output_parser
)

# === LOOP DE CHAT (praticamente sem altera√ß√µes) ===
print("\nüí¨ Chat iniciado! Digite sua pergunta ou 'sair' para encerrar.")
while True:
    pergunta = input("\nVoc√™: ")
    if pergunta.lower() in ["sair", "exit", "quit"]:
        print("üëã Chat encerrado.")
        break

    print("IA:", end=" ", flush=True)
    
    resposta_completa = ""
    for token in chain.stream({"input": pergunta}):
        print(token, end="", flush=True)
        resposta_completa += token
    
    memory.save_context({"input": pergunta}, {"output": resposta_completa})
